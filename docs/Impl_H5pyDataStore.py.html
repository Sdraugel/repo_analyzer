
        <!DOCTYPE html>
        <html lang="en">
        <head>
            <meta charset="UTF-8">
            <meta name="viewport" content="width=device-width, initial-scale=1.0">
            <title>H5pyDataStore.py Details</title>
            <style>
                .container {
                    width: 60%;
                    margin: 50px auto;
                }
                h2 {
                    text-align: center;
                }
                .description {
                    background-color: #f9f9f9;
                    padding: 20px;
                    border: 1px solid #ddd;
                    border-radius: 5px;
                    margin-top: 20px;
                }
                button {
                    padding: 10px 20px;
                    background-color: #4CAF50;
                    color: white;
                    border: none;
                    cursor: pointer;
                }
                button:hover {
                    background-color: #45a049;
                }
            </style>
        </head>
        <body>
            <div class="container">
                <h2>H5pyDataStore.py</h2>
                <div class="description">
                    <p><strong>Description:</strong></p>
                    <p>The provided code is a Python script that seems to be part of a larger software system developed by Raytheon Company. The script appears to be a module for handling data storage, specifically using the HDF5 file format. HDF5 is a data model, library, and file format for storing and managing data. It supports an unlimited variety of datatypes and is designed for flexible and efficient I/O and for high volume and complex data.

The script begins with a detailed comment section that includes the software's history, modifications, and the engineers who made those changes. This is followed by several import statements that bring in necessary Python libraries and modules for the script to function, such as h5py for handling HDF5 files, numpy for numerical operations, and others for file and system operations.

The script also imports several modules from a package named pypies, which seems to be a proprietary package possibly developed by Raytheon or for the specific project this script is part of. The modules imported from pypies seem to be related to data storage and handling.

The script then sets up a logger for logging events or errors, defines a special datatype for handling variable-length strings in HDF5 files, and sets file permissions.

Next, a dictionary named `dataRecordMap` is defined that maps different data record types to their corresponding numpy datatypes. This is likely used when reading from or writing to HDF5 files to ensure data is correctly interpreted.

The script also defines some constants related to chunk size and filesystem block size, which may be used for efficient reading/writing of data. The script ends abruptly, so it's not clear what the full functionality of this script is.

The provided code is a part of a Python program that deals with the storage and management of data in HDF5 format. HDF5 is a data model, library, and file format for storing and managing data. It supports an unlimited variety of datatypes, and is designed for flexible and efficient I/O and for high volume and complex data.

Here's a breakdown of what the code does:

1. It defines two regular expressions (`PURGE_REGEX` and `ORPHAN_REGEX`) to match specific patterns in the filenames. The `PURGE_REGEX` matches a specific pattern of date formats used in filenames, and `ORPHAN_REGEX` matches another date format. 

2. It defines a buffer (`ORPHAN_MOD_BUFFER`) to ignore purging recently-modified files.

3. It defines a class `H5pyDataStore` that inherits from `IDataStore.IDataStore`. This class is responsible for managing the storage of data in HDF5 format.

4. The `store` method in the class is responsible for storing a request. It first checks the disk space, gets the records and metadata from the request, prepares the records for storage, opens the file, writes the data to the HDF file, and finally closes the file and releases the lock.

5. The `__prepareRecordsToStore` method prepares the records for storage by calling the `prepareStore` method on each record.

6. The `__writeHDF` method writes the data to the HDF file. It first checks if the data is chunked and compressed, retrieves the data object from the record, gets the root node of the HDF file, and writes the data to the file.

7. The `__writeHDFDataset` method writes a dataset to the HDF file. It checks if the dataset already exists in the group, and if so, it either appends, replaces, or overwrites the data in the dataset. If the dataset does not exist, it creates a new dataset.

8. The code also handles exceptions and logs warnings and debug information.

This code file appears to be a part of a larger system that works with HDF5 files. HDF5 is a data model, library, and file format for storing and managing data. It supports an unlimited variety of datatypes and is designed for flexible and efficient I/O and for high volume and complex data.

Here's a breakdown of what the code does:

1. `__getHdf5Datatype(self, record)`: This method determines the data type of a given record. If the record's class is in the `dataRecordMap`, it retrieves the corresponding data type. If not, it calls the `determineStorageType()` method on the record to get the data type. If the data type is bytes, it further refines the data type based on the maximum length of the record.

2. `__reverseDimensions(self, dims)`: This method reverses the dimensions of a given array. It's useful when arrays are passed in x/y and the orientation of data is y/x.

3. `__calculateChunk(self, nDims, dataType, storeOp, maxDims)`: This method calculates the chunk size for storing data in the HDF5 file. The chunk size depends on the number of dimensions, the data type, the storage operation, and the maximum dimensions. 

4. `__writeProperties(self, dr, dataset)`: This method writes the properties of a data record to a dataset in the HDF5 file.

5. `__writePartialHDFDataset(self, f, data, dims, szDims, dataset, group, props, dataType, minIndex, maxSizes, fillValue)`: This method writes a portion of a dataset to the HDF5 file. It reshapes the data to match the dimensions, checks if the dataset already exists in the group, and writes the data to the dataset.

6. `delete(self, request)`: This method deletes specified datasets and groups from the HDF5 file. It opens the file in write mode, iterates over the datasets and groups specified in the request, and deletes each one.

The code also handles various exceptions and logs warnings when it encounters issues, such as not being able to find a dataset or group to delete, or trying to replace data of one type with data of a different type.

This code appears to be part of a larger class that manages data stored in HDF5 files. HDF5 is a data model, library, and file format for storing and managing data. It supports an unlimited variety of datatypes, and is designed for flexible and efficient I/O and for high volume and complex data.

Here's a breakdown of what each part of the code is doing:

1. The first part of the code is trying to delete a specific group from an HDF5 file. It logs a warning if the group is not found, and it handles exceptions that might occur during the deletion process.

2. The `__hasDataSet` method checks if a group in the HDF5 file has any datasets. It does this by recursively searching through all the children of the group.

3. The `retrieve` method retrieves data from a specific group in the HDF5 file. It can retrieve all the data from the group or just a specific dataset. The data is returned as a `RetrieveResponse` object.

4. The `__retrieve` and `__retrieveInternal` methods are helper methods used by the `retrieve` method. They retrieve all the datasets from a group and read the data from a specific dataset, respectively.

5. The `retrieveDatasets` method retrieves data from multiple datasets in the HDF5 file. It also logs the time it takes to read the data.

6. The `retrieveGroups` method retrieves data from multiple groups in the HDF5 file.

7. The `getDatasets` method returns the names of all the datasets in a specific group in the HDF5 file.

In all these methods, the HDF5 file is opened and closed within a `try`/`finally` block to ensure that the file is properly closed even if an error occurs. The code also uses a lock to prevent multiple threads from accessing the file at the same time.

The code provided is part of a larger program that manages files and datasets, specifically in the HDF5 format. Here's a breakdown of what each part does:

1. `deleteFiles(self, request)`: This method deletes a file or directory specified by the request. If the file is a directory, it calls a recursive method to delete all files within that directory. If the deletion is successful, it returns a DeleteResponse object with success set to True.

2. `createDataset(self, request)`: This method creates a new dataset in an HDF5 file. It first checks if there's enough disk space for the file. If there isn't, it returns an error response. If there is, it opens the file, gets the properties of the record from the request, and checks if the data needs to be chunked and compressed. It then creates the dataset with the specified properties and writes the properties to the dataset. Finally, it closes the file and releases the lock on it.

3. `__createDatasetInternal(self, group, datasetName, dtype, szDims, maxDims=None, chunks=None, compression=None, fillValue=None)`: This is a helper method that creates a new dataset in an HDF5 file with the specified properties.

4. `__recursiveDeleteFiles(self, dir, datesToDelete)`: This is a helper method that recursively deletes all files in a directory that match the specified dates.

5. `__removeFile(self, path)`: This is a helper method that removes a file at the specified path.

6. `__removeDir(self, path, onlyIfEmpty=False)`: This is a helper method that removes a directory at the specified path.

7. `__openFile(self, filename, mode='r')`: This is a helper method that opens a file with the specified mode and acquires a lock on it.

8. `__getNode(self, rootNode, groupName, dsName=None, create=False)`: This is a helper method that gets a node from the root node of an HDF5 file.

The code also uses a LockManager to manage locks on files, ensuring that only one process can access a file at a time. It also measures the time taken to open and close files.

The code is written in Python and seems to be part of a larger system for managing and manipulating data stored in a hierarchical structure, possibly in an HDF5 file or similar data format. 

The first part of the code defines a method for navigating through a hierarchical data structure. It takes a path to a data set (dsName) within a group (groupName) and returns the corresponding node. If the node does not exist and the `create` flag is set to True, it creates the node. If the node cannot be found and `create` is False, it raises a `StorageException`. It also keeps track of the time taken to get the group.

The `__link` method creates a hard link from a group to a dataset.

The `copy` method copies a file from one location to another. If the file does not exist, it logs an error and returns a `FileActionResponse` with the failed file.

The `__doCopy` method is a helper method for the `copy` method. It copies a file to a specified output directory and changes the file permissions of the copied file.

The `repack` method repacks a list of HDF5 files. It gets the list of files from the `__listHdf5Files` method, which recursively searches a directory for .h5 files.

The `__doRepack` method is a helper method for the `repack` method. It calls the `h5repack` command-line tool to repack an HDF5 file. If the repacking is successful, it replaces the old file. If an error occurs during the repacking, it logs the error and the output of the `h5repack` command.

This code is part of a larger system that performs various operations on files. 

The first part of the code checks if an output directory (`outDir`) is specified. If not, it removes the original file (`filepath`), renames the repacked file to the original file's name, and sets certain permissions on the file. If an output directory is specified, it changes the permissions of the repacked file. If the repacking process fails, it removes the failed new file and copies the original file to the output directory, setting the appropriate permissions.

The `__doRepack` function is given a display name 'repack' for identification purposes.

The `__getFileActionName` function retrieves the display name of the function object passed to it, or its name if no display name is present.

The `__doFileAction` function performs a specified action on a file. It first opens the file and checks if a certain attribute exists and if it's greater than the file's last modified time. If so, it doesn't proceed with the action. Otherwise, it updates the time attribute, performs the action, and updates a response object with the file's status (whether the operation succeeded or failed).

The `deleteOrphanFiles` function starts a process to delete orphan files. It gets a list of files and a map of the oldest dates. It then compiles the keys to a regex and remaps it to the datetime. It seems like the code is cut off, but it appears to be starting a process to walk through a directory and remove orphaned files.

This code is part of a larger program that manages files in a directory. It performs the following tasks:

1. Walks through a directory structure, starting from a specified path, and identifies all files with the '.h5' extension.

2. For each '.h5' file, it checks if the file matches a certain pattern (ORPHAN_REGEX) and if the file has been recently modified. 

3. If the file matches the pattern and hasn't been recently modified, it checks the file's date against a reference date (compareTime). If the file's date is older than the reference date, the file is considered to be "orphaned" and is deleted. If the file doesn't match any known subdirectory pattern and isn't recently modified, it is also considered to be "orphaned" and is deleted.

4. The code also checks for empty directories at each depth of the directory structure and deletes them.

5. After the purge operation, it logs the time taken for the operation and cleans up any locks on the path.

6. It then returns a response with lists of successfully deleted files and files that failed to delete.

7. The `__checkDiskSpace` function checks the disk usage for a given file path. If the disk usage exceeds a certain threshold (fullDiskThreshold), it logs an error message and returns an ErrorResponse.

In summary, this part of the code is responsible for managing storage, specifically purging "orphaned" files and empty directories, and checking disk usage.</p>
                </div>
                <div style="text-align: center; margin-top: 20px;">
                    <a href="javascript:history.back()"><button>Back</button></a>
                </div>
            </div>
        </body>
        </html>
        